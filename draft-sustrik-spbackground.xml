<?xml version="1.0" encoding="US-ASCII"?>
<!DOCTYPE rfc SYSTEM "rfc2629.dtd">

<rfc category="info" docName="draft-sustrik-spbackground-01" ipr="full3978">

  <front>

    <title abbrev="SP Background">
    Background for Scalability Layer/Protocol
    </title>

    <author fullname="Martin Sustrik" initials="M." role="editor"
            surname="Sustrik">
      <address>
        <postal>
          <street>Stropkovska 9</street>
          <code>82103</code>
          <city>Bratislava</city>
          <country>Slovakia</country>
        </postal>
        <phone>+421 908 714 885</phone>
        <email>sustrik@250bpm.com</email>
      </address>
    </author>

    <date month="August" year="2011" />

    <area>Applications</area>
    <workgroup>Internet Engineering Task Force</workgroup>

    <keyword>distributed computing</keyword>
    <keyword>scalability</keyword>

    <abstract>
      <t>TODO</t>
    </abstract>

  </front>

  <middle>

    <section title="Introduction">

      <t>The world of Internet is moving in a very specific direction.
      As clients are almost constantly online, the need for client-side
      software diminishes. Thin client becomes the norm. Users are
      happy to lose the control of their data and software just to get
      rid of the need for administration. Service providers, on the
      other hand, are happy to trade thick clients for extremely low
      time-to-market provided by application executed in the browser,
      with client-side code downloaded just in time from the server.
      </t>

      <t>In short, the world looks more and more like a collection of
      centralised services, each forming its own star-like topology,
      service provider being in the middle and thin clients on the
      edges.</t>

      <t>This trend is further amplified by the fact that no two
      services are exactly the same. There's not much point in designing
      public protocols to allow distribution of a service that's
      provided by a single provider or maybe a handful of providers at
      best.</t>

      <t>IETF already reflects this trend. As 
      <eref target="http://tools.ietf.org/html/draft-tschofenig-post-standardization-00">
      "Web Applications: Trends and Implications"</eref> Internet draft
      says: "In a nutshell, there is a certain class of applications for
      which the standardization need is diminishing: chances are good
      that your standardization work will not be relevant in
      such an environment."</t>

      <t>However, there's a different trend emerging behind the scenes.
      There's an increased need for services to use other services.
      It often makes sense to offload a specific functionality to an
      online service and focus on what you are good at instead of
      competing with existing service providers in the functionality you
      are not expert at. Even more importantly, by using other services
      you can reach the users centered around those services, which in
      turn means getting a traction far more quickly than you would get
      marketing the product in the old-fashioned way. Finally, automated
      access to services allows for combining multiple services in an
      innovative way to deliver added value to the user.</t>

      <t>All these trends are already visible in the online world:
      software-as-a-service paradigm, delivering applications via social
      networking sites, just-in-time serving of ads provided by external
      companies, mashups, migrating data between different online
      services etc.</t>

      <t>Given that services using other services require much more
      precise contract than human beings who are generally all right
      with loosely defined "graphical user interface", these new trends
      lead to re-emergence of protocols, although in a slightly mutated
      manner. Online services already provide "APIs" for developers to
      hook in. Unfortunately, these are entirely ad hoc. The same
      functionality is re-invented and re-implemented over and over
      again. The APIs tend to be buggy and insecure. The interactions
      with underlying network is almost universally ignored.</t>

      <t>Thus, it seems there's a need for standardised
      application-level protocols here, however, not the classic
      application protocols that convey business logic -- the business
      logic is different for each service anyway -- but protocols to
      handle pieces of functionality that are common to large number
      of services.</t>

      <t>Such functionality invariably has to deal with the most
      low-level aspects of the application protocols, namely abstraction
      of the underlying network and interaction with it as well as the
      generic communication patterns between services.</t>

      <t>This memo provides a background, both historic and technical,
      and a rationale for such a layer and discusses some of the
      associated use cases.</t>

    </section>

    <section title="History">

    <t>Historically, there are two large trends in designing the
    generic "middleware" layer that lives between netowrking stack and
    the applications.</t>

    <t>First, there is a tradition of "message-oriented middleware" or
    "enterprise messaging" if you like. It starts in 80's when gradual
    transition from mainframes to PCs triggered the first generation
    of messaging systems.</t>

    <t>The enterprise nature of these system gave rise to their two
    main traits: extensive featureset and assumption of central
    control.</t>

    <t>By the start of new millenia messaging middleware have
    accumulated large set of disparate functionality, ranging from
    data serialisation and persistence (storing message queues in
    a database) to complex message filtering algorithms,  wide
    range of security techniques, different routing algorithms, data
    transformation services, distributed transactions, sophisticated
    tools for governance and monitoring, enterprise application
    integration and so on.</t>

    <t>All the above is further done with the assumption that the whole
    system is deployed by a single entity (company) and that it is
    completely under control of its administrators. Consequently,
    there's little focus on providing reliability and robustness needed
    in loosely coupled, multi-tenant and often malicious Internet
    environment.</t>

    <t>Traditionally, the market have been dominated by proprietary
    products such as TIBCO's RendezVous or IBM's WebSphereMQ. However,
    early 90's witness the attempts to standardise the middleware layer.
    They bring object-orientedness to the design and result in standards
    such as CORBA (1991) and Microsoft's COM (1992). While these
    standards were widely used they haven't replaced the traditional
    business messaging solutions, rather they created a niche of their
    own.</t>

    <t>Standardisation of traditional message-oriented middleware
    began in 2001 with JMS standard describing Java APIs to access
    messaging functionaliy. In 2006, a standrd wire-level protocol for
    enterprise messaging, AMQP, was published.</t>

    <t>As a side note it should be said that HPC industry have developed
    standards of their own. MPI (1994) is a de facto API standard for
    communication between applications running on supercomputers and
    computational clusters. The basic principles are similar to
    enterprise messaging, most importantly the assumption of cetralised
    and fully controled environment.</t>

    <t>The second big trend in messaging arises from the need to use
    software services over the Internet, specifically to use world-wide
    web as a application-to-application messaging platform rather than
    a way to deliver documents and GUIs to the end users.</t>

    <t>When compared to the enterprise messaging, the web services
    infrastructure tends to be more leight-weight on functionality while
    it focuses more on features such as addressing and discovery, that
    are crucial in loosely coupled distributed environment.</t>

    <t>TODO: History of SOAP, WS etc.</t>

    <t>On more theoretical level, SOA (service-oriented architecture)
    is much promoted since beginning of the century as a way to think
    about distributed computing. It views the world as a set of
    coarse-grained interoperable services that can be quickly composed
    into novel applications.</t>

    <t>REST architecture (2000), on the other hand, focuses on the
    concept of "resource" living on the network while distributed
    computing takes the form of accessing and manipulating
    the resource.</t>

    </section>

    <section title="The network">

    <t>Traditionally, interactions between the messaging layer and the
    low-level network stack have been minimal. It is commonly assumed
    that:

    <list style="numbers">
        <t>The performance of messaging is not as critical as to need a
        support from the underlying network (traffic shaping, using
        multicast instead of unicast, setting ToS bits or similar).</t>
        <t>Setting up the messaging infrastructure is not a task for
        the network administrator, rather, it should be completely
        transparent to the network administrator.</t>
    </list>

    </t>

    <t>Thus, it is common that messaging solutions use protocols that
    are not friendly to the underlying network, whether it means using
    formatting that is not easily parsed in the hardware, such as XML,
    or simply funnelling all the disparate messaging traffic through a
    single TCP port, even a single TCP connection, which virtually
    disables any fine-grained monitoring, QoS or traffic shaping on the
    network layer.</t>

    <t>There is one specific breed of messaging solutions though that
    takes the underlying network into account. In stock trading industry
    there's a big emphasis on performance. Microseconds make difference
    between executed and failed trade and translate directly into
    revenue. Thus, the latencies introduced by the network are of
    utmost interest.</t>

    <t>Messaging solutions aimed at stock trading market are rather
    different from the rest of the ecosystem. They often use UDP instead
    of TCP and multicast instead of unicast to disseminate the stock
    quotes. Solutions such as <xref target="refs.RFC3208">PGM</xref>
    are used primarily in stock trading industry. It's even
    common to use specialised hardware devices (network accelerators,
    dedicated FPGAs etc.) to achieve better performance.</t>

    <t>In any case, if we believe that online services are going to
    proliferate in the future and that significant portion of Internet
    traffic is going to be related to online services, ability to
    administer the traffic on the network level is going to become
    increasingly significant.</t>
    
    </section>

    <section anchor="IANA" title="IANA Considerations">
      <t>This memo includes no request to IANA.</t>
    </section>

    <section anchor="Security" title="Security Considerations">
      <t>This memo has no security considerations.</t>
    </section>

  </middle>

  <back>
    <references>
      <reference anchor="refs.RFC3208">
        <front>
            <title>PGM Reliable Transport Protocol Specification</title>
            <author initials="T." surname="Speakman"
                    fullname="T. Speakman et al">
            </author>
            <date month="December" year="2001" />
        </front>
      </reference>
    </references>
  </back>

</rfc>


